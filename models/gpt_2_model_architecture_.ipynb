{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import libraries"
      ],
      "metadata": {
        "id": "CNuD1Z9lTYeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "from dataclasses import dataclass\n",
        "from easy_transformer import EasyTransformer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
        "import tqdm.auto as tqdm"
      ],
      "metadata": {
        "id": "XYRX86OTT3OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print all activation shapes of reference model (for debugging)"
      ],
      "metadata": {
        "id": "5LPqDm8OT62X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for activation_name, activation in cache.cache_dict.items():\n",
        "  if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
        "    print(activation_name, activation.shape)"
      ],
      "metadata": {
        "id": "o2g17hJ_T98b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    d_model: int = 768               # Dimensionality of the model's hidden layer (each token's representation size)\n",
        "    debug: bool = True               # Flag to enable or disable debug mode\n",
        "    layer_norm_eps: float = 1e-5     # Small epsilon value added in layer normalization to avoid division by zero\n",
        "    d_vocab: int = 50257             # Size of the model's vocabulary (number of unique tokens the model can handle)\n",
        "    init_range: float = 0.02         # Standard deviation for initializing model weights\n",
        "    n_ctx: int = 1024                # Maximum sequence length (number of tokens in an input sequence)\n",
        "    d_head: int = 64                 # Dimensionality of each attention head\n",
        "    d_mlp: int = 3072                # Dimensionality of the MLP (feed-forward network) hidden layer\n",
        "    n_heads: int = 12                # Number of attention heads in each transformer block\n",
        "    n_layers: int = 12               # Number of transformer blocks (layers) in the model\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "print(cfg)\n"
      ],
      "metadata": {
        "id": "TDKJYF0XR6zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LayerNorm"
      ],
      "metadata": {
        "id": "iLYY8_qeGK8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Make mean 0\n",
        "\n",
        "2. Normalize to have variance 1\n",
        "\n",
        "3. Scale with learned weights\n",
        "\n",
        "4. Translate with learned bias\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ml5J7uvJRu6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    # Initialize learnable parameters for scaling (w) and bias (b)\n",
        "    self.w = nn.Parameter(torch.ones(cfg.d_model)) # Scaling weight initialized to ones\n",
        "    self.b = nn.Parameter(torch.zeros(cfg.d_model)) # Bias initialized to zeros\n",
        "\n",
        "  def forward(self, residual):\n",
        "    if cfg.debu:print(\"Residual:\", residual.shape)\n",
        "    # Subtract the mean from the residual across the last dimension (d_model)\n",
        "        # This centers the inputs around zero\n",
        "    residual = residual - einops.reduce(\"batch position d_model -> batch position\", residual, \"mean\")\n",
        "    # Calculate variance, square root it. Add in an epsilon to avoid dividing by 0\n",
        "    scale = einops.reduce(\"batch position d_model -> batch position\", residual.pow(2), \"var\") + cfg.layer_norm_eps.sqrt() # to avoid division by zero\n",
        "    # Normalize the residual by dividing by the calculated scale\n",
        "    normalized = residual/scale\n",
        "    # Apply learned scaling (w) and bias (b) to the normalized values\n",
        "    normalized = normalized * self.w + self.b\n",
        "    if cfg.debug: print(\"Normalized:\", residual.shape)\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "4IKVFDf9Rrt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding\n"
      ],
      "metadata": {
        "id": "N_PLAgrJRjZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    # Initialize an embedding weight matrix (W_E) with shape (d_vocab, d_model)\n",
        "    self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
        "    # Fill the weight matrix with normally distributed values\n",
        "    nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "      if cfg.debug: print(\"Tokens:\", tokens.shape)\n",
        "      # Retrieve the embeddings corresponding to the input token IDs\n",
        "      embed = self.W_E[tokens,:]\n",
        "      if cfg.debug: print(\"Embeddings:\", embed.shape)\n",
        "      return embed"
      ],
      "metadata": {
        "id": "RH3jE8VoxznP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XFEEUScnFZN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Embedding"
      ],
      "metadata": {
        "id": "vgXYvvoVRl_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PosEmbed(nn.Module):\n",
        "  '''A lookup table for positional embeddings, providing an embedding vector for each position in the sequence.'''\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    # Store the configuration, which includes parameters like sequence length and embedding dimension.\n",
        "    self.cfg = cfg\n",
        "    # Initialize a learnable positional embedding matrix, W_P, with shape (n_ctx, d_model)\n",
        "        # - n_ctx: maximum sequence length (context size), meaning it provides an embedding for each position up to this length.\n",
        "        # - d_model: embedding size for each position.\n",
        "    self.W_P = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
        "    # Initialize W_P with values drawn from a normal distribution.\n",
        "    # The std (standard deviation) for this distribution is set by cfg.init_range, controlling the spread of initial values.\n",
        "    nn.init.normal_(self.W_P, std=self.cfg.init_range)\n",
        "\n",
        "  def forward(self, tokens):\n",
        "    if cfg.debug: print(\"Tokens:\", tokens.shape)\n",
        "    # Retrieve only the positional embeddings up to the current sequence length.\n",
        "    # This slice has shape (sequence_length, d_model), as it selects positional embeddings\n",
        "    # only for positions up to the sequence length (tokens.size(1)) for each batch.\n",
        "    pos_embed = self.W_P[:tokens.size(1),:]\n",
        "    # Repeat pos_embed across the batch dimension, so it can match the shape of tokens.\n",
        "    # This uses einops to replicate the position embeddings for each sequence in the batch.\n",
        "    # The resulting shape is (batch, sequence_length, d_model).\n",
        "    pos_embed = einops.repeat(pos_embed, \"d_model -> batch position d_model\", batch=tokens.size(0))\n",
        "    if cfg.debug: print(\"pos_embeddings:\", pos_embed.shape)\n",
        "    return pos_embed\n"
      ],
      "metadata": {
        "id": "rj5YhXY00ioC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention"
      ],
      "metadata": {
        "id": "bPmAqZQ6-rZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Produce an attention pattern â€“ for each destination token, probability distribution over previous tokens (incl current token)\n",
        "2 * Linear map from input -> query, key shape[batch, position, head_index, d_head]\n",
        "* Dot product every *pair* of queries and keys to get a[batch, head_indey, query_pos, key_pos] (query = dest, key = source)\n",
        "* Scale and mask attn_scores to make it lower triagular, i.e. causal\n",
        "*softmax row-wise, to get prob distribution along each of the key_pos dimension - attention pattern!\n",
        "\n",
        "**Step 2**: Move information from source tokens to destination token using attention pattern (move=apply linear map)\n",
        "* Linear map from input -> value [batch, key_pos, head_indey, d_head]\n",
        "* Mix along the key_pos with attn pattern to get z, a mixed value [batch, query_pos, head_index, d_head]\n",
        "* Map to output, [batch, position, d_model] (position = query_pos, we've summed over all heads)"
      ],
      "metadata": {
        "id": "uPLZkncH1O-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  '''Attention mechanism for Transformer models.'''\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    # Store config, includes model hyperparameters\n",
        "    self.cfg = cfg\n",
        "    # Initialize weight matrices for query, key and value\n",
        "    self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "    nn.init.normal_(self.W_Q, std= self.cfg.init_range)\n",
        "    self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "    self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "    nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
        "    self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "    self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
        "    nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
        "    self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
        "\n",
        "    # Initialize the output weight matrix (W_O) for combining the results from each attention head.\n",
        "    # Shape: (n_heads, d_head, d_model), mapping each head back to the model dimension.\n",
        "    self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
        "    nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
        "    self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
        "\n",
        "    # Set a large negative value used to mask out future tokens in the attention scores (causal masking).\n",
        "    self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
        "\n",
        "  def forward(self, normalized_resid_pre):\n",
        "    if cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
        "\n",
        "    # Calculate query vectors by applying W_Q to the input and adding bias.\n",
        "    q = einsum(\"batch query_pos n_heads d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
        "    # Calculate key vectors by applying W_K to the input and adding bias.\n",
        "    k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self. b_K\n",
        "\n",
        "    # Apply causal masking to ensure the model only attends to previous and current tokens.\n",
        "    attn_scores = attn_scores/ nath.sqrt(self.cfg.d_head)\n",
        "    attn_scores = self.apply_causal_mask(attn_scores)\n",
        "\n",
        "    # Calculate value vectors by applying W_V to the input and adding bias.\n",
        "    v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
        "\n",
        "    # Use the attention scores to compute the weighted average of the value vectors.\n",
        "    # The attention probabilities (attn) weight the value vectors (v).\n",
        "    z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", attn, v)\n",
        "\n",
        "    # Combine attention head outputs with W_O to project back to model dimension.\n",
        "    attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
        "    return attn_out\n",
        "\n",
        "  def apply_causal_mask(self, attn_scores):\n",
        "    # Causal mask to prevent attending to future tokens.\n",
        "    # We use an upper triangular matrix filled with 1s to mask future positions.\n",
        "    # Positions above the diagonal (future tokens) are masked by setting scores to a very low value (self.IGNORE).\n",
        "    mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1),device = attn_scores.device), diagonal=1).bool()\n",
        "    attn_scores.masked_fill_(mask, self.IGNORE)\n",
        "    return attn_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "hr7Qbin-FH5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP"
      ],
      "metadata": {
        "id": "k2jtoyOjAc-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  '''The MLP class represents a feed-forward network in each transformer layer.\n",
        "  It provides additional non-linear transformations after the attention mechanism.'''\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    # Initialize the weight and bias parameters for the first linear layer, W_in and b_in.\n",
        "    self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
        "    nn.init.normal_(self.W_in, std=self.cfg.init_range) # Randomly initialize with a small standard deviation.\n",
        "    self.b_in = nn.Parameter(torch.zeros(cfg.d_mlp)) # Initialize bias with zeros.\n",
        "\n",
        "    # Initialize the weight and bias parameters for the second linear layer, W_out and b_out.\n",
        "    self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
        "    nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
        "    self.b_out = nn.Parameter(torch.zeros(cfg.d_model))\n",
        "\n",
        "  def forward(self, normalized_resid_mid):\n",
        "    '''Perform the forward pass, which applies the MLP transformation to the input.'''\n",
        "    if cfg.debug: print(\"Normalized_resid_mid:\", normalized_resid_mid.shape)\n",
        "    # First, apply the first linear transformation (W_in) to expand the input to d_mlp dimensions.\n",
        "    pre = einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\", normalized_resid_mid, self.W_in) + self.b_in\n",
        "\n",
        "    # Apply the activation function (GELU), which introduces non-linearity.\n",
        "    # GELU (Gaussian Error Linear Unit) is used because it tends to yield better results in transformer architectures.\n",
        "    post = gelu_new(pre)\n",
        "\n",
        "    # Apply the second linear transformation (W_out) to project back to the model's original dimension (d_model).\n",
        "    # This compresses the intermediate representation back to the original model dimension.\n",
        "    mlp_out = einsum(\"batch position d_mlp, d_mlp d_model -> batch position d_model\", post, self.W_out) + self.b_out\n",
        "    # Return the final output of the MLP layer, which will be added back to the residual stream.\n",
        "    return mlp_out"
      ],
      "metadata": {
        "id": "rHXcRJTQGoqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Block"
      ],
      "metadata": {
        "id": "44G2Zb1EAfrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  ''' '''\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    # Initialize the first LayerNorm, Attention, second LayerNorm, and MLP layers.\n",
        "    # These correspond to the main components in a transformer block.\n",
        "    self.ln1 = LayerNorm(cfg)\n",
        "    self.attn = Attention(cfg)\n",
        "    self.ln2 = LayerNorm(cfg)\n",
        "    self.mlp = MLP(cfg)\n",
        "\n",
        "  def forward(self, resid_pre):\n",
        "    '''resid_pre: The input to this block, containing residual stream information'''\n",
        "    # First, apply layer normalization to the residual stream before feeding it to attention.\n",
        "    normalized_resid_pre = self.ln1(resid_pre)\n",
        "    # Pass the normalized residual through the self-attention layer to get attn_out.\n",
        "    attn_out = self.attn(normalized_resid_pre)\n",
        "    # Add the attention output (attn_out) to the original input (resid_pre) to form resid_mid.\n",
        "    resid_mid = resid_pre + attn_out # This is the first residual connection\n",
        "\n",
        "    # Normalize resid_mid and pass it through the MLP layer.\n",
        "    normalized_resid_mid = self.ln2(resid_mid)\n",
        "    # Pass normalized output to the MLP, which consists of a linear layer, activation, and another linear layer.\n",
        "    mlp_out = self.mlp(normalized_resid_mid)\n",
        "    # Add the MLP output to resid_mid to form the final output, resid_post, for this block.\n",
        "    resid_post = resid_mid + mlp_out # second residual connection\n",
        "    # Return the final residual post-layer (resid_post), which will be the input to the next block.\n",
        "    return resid_post"
      ],
      "metadata": {
        "id": "1KYYEhA4HYt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unembedding"
      ],
      "metadata": {
        "id": "dXRKrXWnAjAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unembed(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    # Define the unembedding weight matrix W_U, which maps from the model's hidden dimension to the vocabulary dimension\n",
        "    self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
        "    # Initialize W_U with a normal distribution, scaled by the initial range defined in cfg\n",
        "    nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
        "    # Define a bias term b_U for each vocabulary token. By setting `requires_grad=False`,\n",
        "    # this bias won't be trained (optional for specific designs).\n",
        "    self.b_U = nn.Parameter(torch.zeros(cfg.d_vocab), requires_grad=False)\n",
        "\n",
        "    def forward(self, normalized_resid_final):\n",
        "      if cfg.debug: print(\"Normalized_resid_final:\", normalized_resid_final.shape)\n",
        "      # Matrix multiplication between the hidden state (`normalized_resid_final`) and `W_U`.\n",
        "      # This transforms the hidden state into logits over the vocabulary size.\n",
        "      logits = einsum(\"batch position d_model, d_model d_vocab -> batch position d_vocab\", normalized_resid_final, self.W_U) + self.b_U\n",
        "      return logits"
      ],
      "metadata": {
        "id": "s9u2WLbgIA1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Full Transformer"
      ],
      "metadata": {
        "id": "dzoE0NBlAnTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kofr-JHV7QrG"
      },
      "outputs": [],
      "source": [
        "class DemoTransformer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.cfg = cfg\n",
        "    # Embedding layer that converts tokens to vectors of size d_model\n",
        "    self.embed = Embed(cfg)\n",
        "    # Positional embedding layer to give the model a notion of word order\n",
        "    self.pos_embed = PosEmbed(cfg)\n",
        "    # Stack of transformer blocks, each adding complexity to the model with self-attention and MLP\n",
        "    self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
        "    # Final layer normalization to stabilize output\n",
        "    self.ln_final = LayerNorm(cfg)\n",
        "    # Unmebedding layer to map final hidden states back to vocabulary logits\n",
        "    self.unembed = Unembed(cfg)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "      # Convert tokens to embeddings\n",
        "      embed = self.embed(tokens)\n",
        "      # get positional embeddings for each position in sequence\n",
        "      pos_embed = self.pos_embed(tokens)\n",
        "      # combine token embeddings with positional embeddings to form initial residual stream\n",
        "      residual = embed + pos_embed\n",
        "\n",
        "      # pass combined embedding through each transformer block\n",
        "      for block in self.blocks:\n",
        "        residual = block(residual)\n",
        "\n",
        "      # apply final layer normalization to output of last transformer block\n",
        "      normalized_resid_final = self.ln_final(residual)\n",
        "      # map final normalized residuals to logits over vocabulary\n",
        "      logits = self.unembed(normalized_resid_final)\n",
        "      return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Entropy Loss"
      ],
      "metadata": {
        "id": "oxujy7g2JTeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lm_cross_entropy_loss(logits, tokens):\n",
        "    # Measure next token loss\n",
        "    # Logits have shape [batch, position, d_vocab]\n",
        "    # Tokens have shape [batch, position]\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
        "    return -pred_log_probs.mean()\n",
        "loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
        "print(loss)\n",
        "print(\"Loss as average prob\", (-loss).exp())\n",
        "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
        "print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))"
      ],
      "metadata": {
        "id": "GY2vXMwfJV_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "C41gkcPVJgN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "w8bSbb3rKJeU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "87obQ4JzJvyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Model"
      ],
      "metadata": {
        "id": "MWczeXh_KMIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DemoTransformer(cfg)"
      ],
      "metadata": {
        "id": "LWFRcGOKJibF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "-KsOapZFKHCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "UI0RjteEJ23l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Training Loop"
      ],
      "metadata": {
        "id": "0IwmKxYjJ_T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "print(\"Number of batches:\", len(data_loader))\n",
        "for epoch in range(num_epochs):\n",
        "    for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
        "        tokens = batch['tokens'].cuda()\n",
        "        logits = model(tokens)\n",
        "        loss = lm_cross_entropy_loss(logits, tokens)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        losses.append(loss.item())\n",
        "        if c % log_every == 0:\n",
        "            print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
        "        if c > max_steps:\n",
        "            break"
      ],
      "metadata": {
        "id": "-O_2vFIUJ-8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}